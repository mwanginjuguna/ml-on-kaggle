{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11011605,"sourceType":"datasetVersion","datasetId":6855911},{"sourceId":11011766,"sourceType":"datasetVersion","datasetId":6856015}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Keyword Extraction Experiment w/ Yake, Rake, TF-IDF\n\n- TF-IDF (Term Frequency-Inverse Document Frequency)\n- YAKE (Yet Another Keyword Extractor)\n- RAKE (Rapid Automatic Keyword Extraction)\n\nThe goal is to see if given a random assignment question, I can get a title that most effectively captures the keywords of the assignment. These keywords should help discover titles that perform better SEO wise, and for search (Google) results page ranking.\n\nThe question is: How can we generate the most efficient title for the given text? In this case, the text is an assignment instructions.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:33:05.420641Z","iopub.execute_input":"2025-03-13T10:33:05.421046Z","iopub.status.idle":"2025-03-13T10:33:05.431858Z","shell.execute_reply.started":"2025-03-13T10:33:05.421019Z","shell.execute_reply":"2025-03-13T10:33:05.430734Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/question1/q1.txt\n/kaggle/input/question/q.txt\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Step 1: Install required packages\n!pip install PyMuPDF rake-nltk yake scikit-learn\n\n# Step 2: Import libraries\nimport nltk\n# Download all required NLTK resources\nnltk.download(['stopwords', 'punkt', 'punkt_tab', 'wordnet', 'omw-1.4'])\n\n\nimport fitz  # PyMuPDF\nimport re\nfrom rake_nltk import Rake\nfrom yake import KeywordExtractor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\n\n# Step 3: Document scraping function\ndef scrape_pdf_text(path):\n    doc = fitz.open(path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:33:05.433735Z","iopub.execute_input":"2025-03-13T10:33:05.434057Z","iopub.status.idle":"2025-03-13T10:33:09.905122Z","shell.execute_reply.started":"2025-03-13T10:33:05.434030Z","shell.execute_reply":"2025-03-13T10:33:09.903566Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.25.3)\nRequirement already satisfied: rake-nltk in /usr/local/lib/python3.10/dist-packages (1.0.6)\nRequirement already satisfied: yake in /usr/local/lib/python3.10/dist-packages (0.4.8)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake-nltk) (3.9.1)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.9.0)\nRequirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake) (8.1.7)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yake) (1.26.4)\nRequirement already satisfied: segtok in /usr/local/lib/python3.10/dist-packages (from yake) (1.5.11)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.4.2)\nRequirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake) (1.1.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->yake) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->yake) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->yake) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->yake) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->yake) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->yake) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->yake) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->yake) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->yake) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->yake) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->yake) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"**Load the question from the file.**","metadata":{}},{"cell_type":"code","source":"\n# Example document (replace with your document path)\ntext = scrape_pdf_text(\"/kaggle/input/question/q.txt\")  # Use any PDF in Kaggle input\n\ntext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:33:55.800107Z","iopub.execute_input":"2025-03-13T10:33:55.800467Z","iopub.status.idle":"2025-03-13T10:33:55.813352Z","shell.execute_reply.started":"2025-03-13T10:33:55.800439Z","shell.execute_reply":"2025-03-13T10:33:55.812169Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'Write a research paper that contains the following: \\nDefine and describe cloud-based collaboration.   \\nGoogle Docs is a cloud-based tool used for document\\nsharing.  \\nDiscuss pros and cons of using Google Docs for\\nbusiness-based documents.   \\nCompare and contrast the use of Google Docs with\\nMicrosoft 365 Word Docs for business-based documents.\\nResearch Paper Requirements:  \\nThe paper should be four pages long, not including\\nthe title and reference pages. \\nUse Times New Roman, size 12 font throughout the\\npaper. \\nApply APA 7th edition style and include three major\\nsections: the Title Page, Main Body, and References.\\nA minimum of two scholarly journal articles (besides\\nyour textbook) are required.\\nWriting should demonstrate a thorough understanding\\nof the materials and address all required elements. \\nWriting should use exceptional language that\\nskillfully communicates meaning to the readers with\\nclarity and fluency and is virtually error-free. \\nNote: plagiarism check required, APA7 format, include\\nReferences, within 8hrs \\n'"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"\n# Download nltk sentence tokenizer data (run once)\n# import nltk\n# nltk.download('punkt')\n\ndef generate_title_sentence_segment(content):\n    sentences = sent_tokenize(content)\n    if sentences:\n        return sentences[0] # Use the first sentence\n    return \"No Title Extracted\" # Handle cases with no sentences\n\n# Load your CSV\n# df = pd.read_csv('daily_questions.csv')\n\n# Generate optimized titles\n# df['optimized_title'] = df['content'].apply(generate_title_sentence_segment)\n\n# Save the updated CSV (optional)\n# df.to_csv('daily_questions_with_titles_sentence.csv', index=False)\n\n# print(df[['title_raw', 'optimized_title']].head()) # Show a preview\noptimized_title = generate_title_sentence_segment(text)\noptimized_title","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:34:04.662974Z","iopub.execute_input":"2025-03-13T10:34:04.663335Z","iopub.status.idle":"2025-03-13T10:34:04.671294Z","shell.execute_reply.started":"2025-03-13T10:34:04.663307Z","shell.execute_reply":"2025-03-13T10:34:04.670331Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'Write a research paper that contains the following: \\nDefine and describe cloud-based collaboration.'"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# RAKE for Phrase Ranking:\ndef generate_title_rake(content):\n    r = Rake()\n    r.extract_keywords_from_text(content)\n    ranked_phrases = r.get_ranked_phrases() # Get ranked phrases\n    if ranked_phrases:\n        return ranked_phrases[:2] # Use the top-ranked phrase\n    return \"No Title Extracted\"\n\nrake_title = generate_title_rake(text)\nrake_title","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:34:15.160646Z","iopub.execute_input":"2025-03-13T10:34:15.160990Z","iopub.status.idle":"2025-03-13T10:34:15.170166Z","shell.execute_reply.started":"2025-03-13T10:34:15.160964Z","shell.execute_reply":"2025-03-13T10:34:15.169019Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"['apply apa 7th edition style', 'two scholarly journal articles']"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# YAKE for Phrase Ranking:\ndef generate_title_yake(content):\n    kw_extractor = KeywordExtractor()\n    keywords = kw_extractor.extract_keywords(content) # Extract keywords (phrase, score)\n    if keywords:\n        return keywords # Use the top-ranked phrase (keyword)\n    return \"No Title Extracted\"\n\nyake_title = generate_title_yake(text)\nyake_title","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:34:22.563670Z","iopub.execute_input":"2025-03-13T10:34:22.564056Z","iopub.status.idle":"2025-03-13T10:34:22.655803Z","shell.execute_reply.started":"2025-03-13T10:34:22.564026Z","shell.execute_reply":"2025-03-13T10:34:22.654675Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"[('describe cloud-based collaboration', 0.01139419529487205),\n ('Define and describe', 0.013083010864350314),\n ('Google Docs', 0.018001472449993478),\n ('Docs', 0.05241615149759153),\n ('cloud-based collaboration', 0.05280622031964822),\n ('describe cloud-based', 0.062238267008141106),\n ('Define', 0.06480294082007379),\n ('Word Docs', 0.07411097376975266),\n ('Google', 0.08145926305964542),\n ('Docs for business-based', 0.09989466125910407),\n ('paper', 0.10347887217316704),\n ('Research Paper Requirements', 0.10370021410516542),\n ('business-based documents', 0.10701176040443292),\n ('research paper', 0.11648890796419961),\n ('cloud-based', 0.14700685900316215),\n ('cloud-based tool', 0.15782633142146119),\n ('Write a research', 0.16766439611237663),\n ('Main Body', 0.169410106738005),\n ('Paper Requirements', 0.16963832012154287),\n ('Times New Roman', 0.17055435761718224)]"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk import ngrams\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:34:35.922954Z","iopub.execute_input":"2025-03-13T10:34:35.923308Z","iopub.status.idle":"2025-03-13T10:34:35.928540Z","shell.execute_reply.started":"2025-03-13T10:34:35.923281Z","shell.execute_reply":"2025-03-13T10:34:35.927082Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"\n# Download nltk tokenizer data (run once if you haven't already)\n# import nltk\n# nltk.download('punkt')\n\ndef generate_title_ngram_freq(content, n=2): # Bigrams by default\n    tokens = word_tokenize(content.lower()) # Tokenize and lowercase\n    bigrams = ngrams(tokens, n)\n    bigram_counts = Counter(bigrams)\n    most_common_bigram = bigram_counts.most_common(5) # Get most frequent bigram\n    if most_common_bigram:\n        return most_common_bigram # Join bigram tuple to string\n    return \"No Title Extracted\"\n\nngram_title = generate_title_ngram_freq(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:34:40.717355Z","iopub.execute_input":"2025-03-13T10:34:40.717749Z","iopub.status.idle":"2025-03-13T10:34:40.725726Z","shell.execute_reply.started":"2025-03-13T10:34:40.717718Z","shell.execute_reply":"2025-03-13T10:34:40.724292Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"ngram_title","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:34:44.849415Z","iopub.execute_input":"2025-03-13T10:34:44.849818Z","iopub.status.idle":"2025-03-13T10:34:44.856213Z","shell.execute_reply.started":"2025-03-13T10:34:44.849788Z","shell.execute_reply":"2025-03-13T10:34:44.855128Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"[(('google', 'docs'), 3),\n (('research', 'paper'), 2),\n (('docs', 'for'), 2),\n (('for', 'business-based'), 2),\n (('business-based', 'documents'), 2)]"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"Pre-process text (sort of normalization equivalent)","metadata":{}},{"cell_type":"code","source":"# Step 4: Text preprocessing\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    stop_words = set(stopwords.words('english'))\n    words = nltk.word_tokenize(text)\n    return ' '.join([word for word in words if word not in stop_words and len(word) > 2])\n\ncleaned_text = preprocess_text(text)\ncleaned_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:34:51.226144Z","iopub.execute_input":"2025-03-13T10:34:51.226465Z","iopub.status.idle":"2025-03-13T10:34:51.236515Z","shell.execute_reply.started":"2025-03-13T10:34:51.226441Z","shell.execute_reply":"2025-03-13T10:34:51.235395Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'write research paper contains following define describe cloudbased collaboration google docs cloudbased tool used document sharing discuss pros cons using google docs businessbased documents compare contrast use google docs microsoft 365 word docs businessbased documents research paper requirements paper four pages long including title reference pages use times new roman size font throughout paper apply apa 7th edition style include three major sections title page main body references minimum two scholarly journal articles besides textbook required writing demonstrate thorough understanding materials address required elements writing use exceptional language skillfully communicates meaning readers clarity fluency virtually errorfree note plagiarism check required apa7 format include references within 8hrs'"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"# Step 5: Keyword extraction implementations\n\n# 5.1 RAKE Implementation\nrake = Rake()\nrake.extract_keywords_from_text(text)\nrake_keywords = rake.get_ranked_phrases()[:10]\nrake_keywords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:37:52.444574Z","iopub.execute_input":"2025-03-13T10:37:52.444932Z","iopub.status.idle":"2025-03-13T10:37:52.454388Z","shell.execute_reply.started":"2025-03-13T10:37:52.444906Z","shell.execute_reply":"2025-03-13T10:37:52.453183Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"['apply apa 7th edition style',\n 'two scholarly journal articles',\n 'size 12 font throughout',\n 'include three major sections',\n 'microsoft 365 word docs',\n 'use times new roman',\n 'skillfully communicates meaning',\n 'use exceptional language',\n 'four pages long',\n 'based tool used']"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"rake.get_ranked_phrases()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:38:00.177040Z","iopub.execute_input":"2025-03-13T10:38:00.177375Z","iopub.status.idle":"2025-03-13T10:38:00.184903Z","shell.execute_reply.started":"2025-03-13T10:38:00.177349Z","shell.execute_reply":"2025-03-13T10:38:00.183510Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"['apply apa 7th edition style',\n 'two scholarly journal articles',\n 'size 12 font throughout',\n 'include three major sections',\n 'microsoft 365 word docs',\n 'use times new roman',\n 'skillfully communicates meaning',\n 'use exceptional language',\n 'four pages long',\n 'based tool used',\n 'using google docs',\n 'plagiarism check required',\n 'research paper requirements',\n 'google docs',\n 'google docs',\n 'reference pages',\n 'include references',\n 'research paper',\n 'based documents',\n 'based documents',\n 'based collaboration',\n 'within 8hrs',\n 'virtually error',\n 'thorough understanding',\n 'required elements',\n 'main body',\n 'document sharing',\n 'discuss pros',\n 'apa7 format',\n 'title page',\n 'describe cloud',\n 'use',\n 'required',\n 'paper',\n 'paper',\n 'title',\n 'references',\n 'cloud',\n 'writing',\n 'writing',\n 'write',\n 'textbook',\n 'readers',\n 'note',\n 'minimum',\n 'materials',\n 'including',\n 'free',\n 'following',\n 'fluency',\n 'demonstrate',\n 'define',\n 'contrast',\n 'contains',\n 'cons',\n 'compare',\n 'clarity',\n 'business',\n 'business',\n 'besides',\n 'address']"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"# 5.2 YAKE Implementation\nyake = KeywordExtractor(lan=\"en\", top=20)\nyake_keywords = yake.extract_keywords(cleaned_text)\nyake_keywords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:35:15.881536Z","iopub.execute_input":"2025-03-13T10:35:15.882005Z","iopub.status.idle":"2025-03-13T10:35:16.060247Z","shell.execute_reply.started":"2025-03-13T10:35:15.881975Z","shell.execute_reply":"2025-03-13T10:35:16.059177Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"[('docs businessbased documents', 0.0011432701183019271),\n ('word docs businessbased', 0.0012491307146187405),\n ('google docs microsoft', 0.0017235193912572293),\n ('google docs businessbased', 0.0018582265824709117),\n ('edition style include', 0.0018696597986292672),\n ('sharing discuss pros', 0.002017488396269081),\n ('discuss pros cons', 0.002017488396269081),\n ('roman size font', 0.002017488396269081),\n ('scholarly journal articles', 0.002017488396269081),\n ('understanding materials address', 0.002017488396269081),\n ('exceptional language skillfully', 0.002017488396269081),\n ('language skillfully communicates', 0.002017488396269081),\n ('skillfully communicates meaning', 0.002017488396269081),\n ('communicates meaning readers', 0.002017488396269081),\n ('meaning readers clarity', 0.002017488396269081),\n ('readers clarity fluency', 0.002017488396269081),\n ('clarity fluency virtually', 0.002017488396269081),\n ('fluency virtually errorfree', 0.002017488396269081),\n ('virtually errorfree note', 0.002017488396269081),\n ('errorfree note plagiarism', 0.002017488396269081)]"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"# 5.3 TF-IDF Implementation (Scikit-learn)\ndef tfidf_extractor(text, n=30):\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n    tfidf_matrix = vectorizer.fit_transform([text])\n    feature_names = vectorizer.get_feature_names_out()\n    return sorted(zip(feature_names, tfidf_matrix.sum(0).A1), \n                  key=lambda x: x[1], reverse=True)[:n]\n\ntfidf_keywords = tfidf_extractor(cleaned_text)\ntfidf_keywords\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:38:23.167435Z","iopub.execute_input":"2025-03-13T10:38:23.167880Z","iopub.status.idle":"2025-03-13T10:38:23.182787Z","shell.execute_reply.started":"2025-03-13T10:38:23.167848Z","shell.execute_reply":"2025-03-13T10:38:23.181361Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"[('docs', 0.2377753193111056),\n ('paper', 0.2377753193111056),\n ('google', 0.1783314894833292),\n ('google docs', 0.1783314894833292),\n ('required', 0.1783314894833292),\n ('use', 0.1783314894833292),\n ('businessbased', 0.1188876596555528),\n ('businessbased documents', 0.1188876596555528),\n ('cloudbased', 0.1188876596555528),\n ('docs businessbased', 0.1188876596555528),\n ('documents', 0.1188876596555528),\n ('include', 0.1188876596555528),\n ('pages', 0.1188876596555528),\n ('references', 0.1188876596555528),\n ('research', 0.1188876596555528),\n ('research paper', 0.1188876596555528),\n ('title', 0.1188876596555528),\n ('writing', 0.1188876596555528),\n ('365', 0.0594438298277764),\n ('365 word', 0.0594438298277764),\n ('7th', 0.0594438298277764),\n ('7th edition', 0.0594438298277764),\n ('8hrs', 0.0594438298277764),\n ('address', 0.0594438298277764),\n ('address required', 0.0594438298277764),\n ('apa', 0.0594438298277764),\n ('apa 7th', 0.0594438298277764),\n ('apa7', 0.0594438298277764),\n ('apa7 format', 0.0594438298277764),\n ('apply', 0.0594438298277764)]"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"# Step 6: Display results\n# print(\"RAKE Keywords:\", [phrase for score, phrase in rake_keywords])\nprint(\"\\nYAKE Keywords:\", [kw[0] for kw in yake_keywords])\nprint(\"\\nTF-IDF Keywords:\", [kw[0] for kw in tfidf_keywords])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:39:09.532214Z","iopub.execute_input":"2025-03-13T10:39:09.532639Z","iopub.status.idle":"2025-03-13T10:39:09.539875Z","shell.execute_reply.started":"2025-03-13T10:39:09.532608Z","shell.execute_reply":"2025-03-13T10:39:09.538602Z"}},"outputs":[{"name":"stdout","text":"\nYAKE Keywords: ['docs businessbased documents', 'word docs businessbased', 'google docs microsoft', 'google docs businessbased', 'edition style include', 'sharing discuss pros', 'discuss pros cons', 'roman size font', 'scholarly journal articles', 'understanding materials address', 'exceptional language skillfully', 'language skillfully communicates', 'skillfully communicates meaning', 'communicates meaning readers', 'meaning readers clarity', 'readers clarity fluency', 'clarity fluency virtually', 'fluency virtually errorfree', 'virtually errorfree note', 'errorfree note plagiarism']\n\nTF-IDF Keywords: ['docs', 'paper', 'google', 'google docs', 'required', 'use', 'businessbased', 'businessbased documents', 'cloudbased', 'docs businessbased', 'documents', 'include', 'pages', 'references', 'research', 'research paper', 'title', 'writing', '365', '365 word', '7th', '7th edition', '8hrs', 'address', 'address required', 'apa', 'apa 7th', 'apa7', 'apa7 format', 'apply']\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"\n# Step 7: Title generation\ndef generate_title(keywords_list):\n    # Simple strategy: Take first keyword from each method\n    return ' '.join([keywords_list[0][0], keywords_list[1][0]])\n\ntitle = generate_title([\n    # [phrase for score, phrase in rake_keywords],\n    [kw[0] for kw in yake_keywords],\n    [kw[0] for kw in tfidf_keywords]\n])\n\nprint(\"\\nGenerated Title:\", title)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:39:56.932330Z","iopub.execute_input":"2025-03-13T10:39:56.932724Z","iopub.status.idle":"2025-03-13T10:39:56.940024Z","shell.execute_reply.started":"2025-03-13T10:39:56.932696Z","shell.execute_reply":"2025-03-13T10:39:56.938457Z"}},"outputs":[{"name":"stdout","text":"\nGenerated Title: docs businessbased documents docs\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"### Preliminary Conclusions\nKeyword extraction is not the right approach to generate any meaningful title from the content provided. DL is needed here.","metadata":{}},{"cell_type":"markdown","source":"End.","metadata":{}}]}